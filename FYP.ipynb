{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Bhavya\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import nltk\n",
    "from gensim.models import Word2Vec\n",
    "from nltk.tokenize import word_tokenize\n",
    "import seaborn as sns\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "nltk.download('punkt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset for TED youtube\n",
    "ted_data = pd.read_csv(\"./Datasets/TED-ED_youtube_metadata.csv\")\n",
    "\n",
    "# Initialize the TF-IDF vectorizer\n",
    "ted_tfidf_vectorizer = TfidfVectorizer(max_features=5000, stop_words='english')\n",
    "\n",
    "# Fit the vectorizer and transform the transcript data\n",
    "ted_tfidf_matrix = ted_tfidf_vectorizer.fit_transform(ted_data['transcript'])\n",
    "\n",
    "# Convert the TF-IDF matrix to a DataFrame\n",
    "ted_tfidf_df = pd.DataFrame(ted_tfidf_matrix.toarray(), columns=ted_tfidf_vectorizer.get_feature_names_out())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset for trending Youtube videos\n",
    "yt_data = pd.read_csv(\"./Datasets/GB_youtube_filtered_dataset.csv\")\n",
    "\n",
    "# Dropping rows with empty transcript values\n",
    "yt_data = yt_data.dropna(subset=['transcript'])\n",
    "\n",
    "# Initialize the TF-IDF vectorizer\n",
    "yt_tfidf_vectorizer = TfidfVectorizer(max_features=5000, stop_words='english')\n",
    "\n",
    "# Fit the vectorizer and transform the transcript data\n",
    "yt_tfidf_matrix = yt_tfidf_vectorizer.fit_transform(yt_data['transcript'])\n",
    "\n",
    "# Convert the TF-IDF matrix to a DataFrame\n",
    "yt_tfidf_df = pd.DataFrame(yt_tfidf_matrix.toarray(), columns=yt_tfidf_vectorizer.get_feature_names_out())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding mean of TF-IDF values without 0 to prevent the mean to be close to 0\n",
    "def tf_idf_mean(row):\n",
    "    non_zero_values = row[row != 0]  # Select non-zero values\n",
    "    if len(non_zero_values) > 0:\n",
    "        return non_zero_values.mean()  # Calculate mean excluding zeros\n",
    "    else:\n",
    "        return 0  # Return 0 if all values are 0\n",
    "\n",
    "# Apply the function row-wise to create a new column\n",
    "yt_tfidf_df['tf_idf_mean'] = yt_tfidf_df.apply(tf_idf_mean, axis=1)\n",
    "ted_tfidf_df['tf_idf_mean'] = ted_tfidf_df.apply(tf_idf_mean, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating combined entertainment dataset and educational datasets\n",
    "yt_entertainment_data = yt_data[yt_data['categoryId'] == 24]\n",
    "yt_entertainment_data = yt_entertainment_data.reset_index(drop=1)\n",
    "\n",
    "# Deleting unnecessary columns\n",
    "yt_entertainment_data.drop(columns=['video_id','categoryId','dislikes'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the density plot\n",
    "plt.figure(figsize=(10,6))\n",
    "ted_tfidf_df['tf_idf_mean'].plot(kind='density', label = 'TED')\n",
    "yt_tfidf_df['tf_idf_mean'].plot(kind='density', label='YouTube Trending')\n",
    "plt.title('TF-IDF Density Plot')\n",
    "plt.xlabel('TF-IDF Score')\n",
    "plt.ylabel('Density')\n",
    "plt.xlim(0, 0.2)\n",
    "plt.ylim(0,)\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word2Vec Approach and it's analysis\n",
    "\n",
    "ted_word2vec_df = pd.DataFrame()\n",
    "\n",
    "# Tokenize the transcripts\n",
    "ted_word2vec_df['tokenized_transcript'] = ted_data['transcript'].apply(lambda x: word_tokenize(str(x).lower()))\n",
    "\n",
    "# Train Word2Vec model on the tokenized transcripts\n",
    "model = Word2Vec(sentences=ted_word2vec_df['tokenized_transcript'], vector_size=100, window=5, min_count=1, workers=4)\n",
    "\n",
    "# Average the word embeddings for each transcript\n",
    "def average_word_vectors(tokens, model, vocabulary, vector_size):\n",
    "    if len(tokens) < 1:\n",
    "        return np.zeros(vector_size)\n",
    "    vectors = [model.wv[word] for word in tokens if word in vocabulary]\n",
    "    if len(vectors) < 1:\n",
    "        return np.zeros(vector_size)\n",
    "    return np.mean(vectors, axis=0)\n",
    "\n",
    "ted_word2vec_df['embedding'] = ted_word2vec_df['tokenized_transcript'].apply(lambda x: average_word_vectors(x, model, model.wv.index_to_key, model.vector_size))\n",
    "\n",
    "# Finding mean of the embeddings: any logic to combine the vector\n",
    "ted_word2vec_df['embedding'].mean()\n",
    "ted_word2vec_df['embedding_mean'] = ted_word2vec_df['embedding'].apply(lambda x: pd.Series(x).mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation matrix visualisation\n",
    "\n",
    "# Merge the dataframes\n",
    "ted_merged_df = pd.concat([ted_word2vec_df, ted_tfidf_df, ted_data], axis=1)\n",
    "ted_merged_df = ted_merged_df[['title','transcript','embedding_mean', 'tf_idf_mean', 'like_count', 'view_count', 'comment_count']]\n",
    "\n",
    "ted_merged_df['like_view_ratio'] = ted_merged_df['like_count'] / ted_merged_df['view_count']\n",
    "ted_merged_df['comment_view_ratio'] = ted_merged_df['comment_count'] / ted_merged_df['view_count']\n",
    "ted_merged_df['comment_like_ratio'] = ted_merged_df['comment_count'] / ted_merged_df['like_count']\n",
    "\n",
    "# Compute correlation matrix\n",
    "correlation_matrix = ted_merged_df[['embedding_mean', 'tf_idf_mean', 'like_count', 'view_count', 'comment_count','like_view_ratio','comment_view_ratio','comment_like_ratio']].corr()\n",
    "print(correlation_matrix)\n",
    "# Plot correlation matrix using heatmap\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=\".2f\", vmin=-1, vmax=1)\n",
    "plt.title('Correlation Between Metrics')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word2Vec Approach and it's analysis\n",
    "\n",
    "yt_word2vec_df = pd.DataFrame()\n",
    "# Tokenize the transcripts\n",
    "yt_word2vec_df['tokenized_transcript'] = yt_data['transcript'].apply(lambda x: word_tokenize(str(x).lower()))\n",
    "\n",
    "# Train Word2Vec model on the tokenized transcripts\n",
    "model = Word2Vec(sentences=yt_word2vec_df['tokenized_transcript'], vector_size=100, window=5, min_count=1, workers=4)\n",
    "\n",
    "# Average the word embeddings for each transcript\n",
    "def average_word_vectors(tokens, model, vocabulary, vector_size):\n",
    "    if len(tokens) < 1:\n",
    "        return np.zeros(vector_size)\n",
    "    vectors = [model.wv[word] for word in tokens if word in vocabulary]\n",
    "    if len(vectors) < 1:\n",
    "        return np.zeros(vector_size)\n",
    "    return np.mean(vectors, axis=0)\n",
    "\n",
    "yt_word2vec_df['embedding'] = yt_word2vec_df['tokenized_transcript'].apply(lambda x: average_word_vectors(x, model, model.wv.index_to_key, model.vector_size))\n",
    "\n",
    "# Finding mean of the embeddings: any logic to combine the vector\n",
    "yt_word2vec_df['embedding'].mean()\n",
    "yt_word2vec_df['embedding_mean'] = yt_word2vec_df['embedding'].apply(lambda x: pd.Series(x).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation matrix visualisation\n",
    "\n",
    "# Merge the dataframes\n",
    "yt_merged_df = pd.concat([yt_word2vec_df, yt_tfidf_df, yt_data], axis=1)\n",
    "yt_merged_df = yt_merged_df[['title','transcript','embedding_mean', 'tf_idf_mean', 'like_count', 'view_count', 'comment_count']]\n",
    "yt_merged_df.head()\n",
    "yt_merged_df['like_view_ratio'] = yt_merged_df['like_count'] / yt_merged_df['view_count']\n",
    "yt_merged_df['comment_view_ratio'] = yt_merged_df['comment_count'] / yt_merged_df['view_count']\n",
    "yt_merged_df['comment_like_ratio'] = yt_merged_df['comment_count'] / yt_merged_df['like_count']\n",
    "\n",
    "# Compute correlation matrix\n",
    "correlation_matrix = yt_merged_df[['embedding_mean', 'tf_idf_mean', 'like_count', 'view_count', 'comment_count','like_view_ratio','comment_view_ratio','comment_like_ratio']].corr()\n",
    "print(correlation_matrix)\n",
    "# Plot correlation matrix using heatmap\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=\".2f\", vmin=-1, vmax=1)\n",
    "plt.title('Correlation Between Metrics')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combining dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combining datasets \n",
    "\n",
    "# Importing other useful datasets\n",
    "yt_educational_data = pd.read_csv(\"./Datasets/educational_youtube_metadata.csv\")\n",
    "vox_data = pd.read_csv(\"./Datasets/vox_youtube_metadata.csv\")\n",
    "\n",
    "# Entertainment \n",
    "entertainment_data = yt_entertainment_data.copy()\n",
    "entertainment_data['label'] = 'entertainment'\n",
    "\n",
    "# Informational\n",
    "informational_data = pd.concat([ted_data,yt_educational_data,vox_data], ignore_index=True, join='inner')\n",
    "informational_data['label'] = 'informative'\n",
    "\n",
    "combined_data = pd.concat([entertainment_data,informational_data], ignore_index=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        i have been gone off youtube for over one mont...\n",
       "1        hi sisters james charles here and welcome back...\n",
       "2        my [Music] [Music] [Music] [Applause] [Music] ...\n",
       "3        i think it's the heat i think it needs to go i...\n",
       "4        what's going on live fam i hope everybody's do...\n",
       "                               ...                        \n",
       "13812    The United States' national debt is $12.5 tril...\n",
       "13813    what have we made progress in that people don'...\n",
       "13814    a mass extinction is just defined as a moment ...\n",
       "13815    \"Obamacare\" \"The obamacare sign-up deadline ge...\n",
       "13816    There's a problem in journalism. We call some ...\n",
       "Name: transcript, Length: 13817, dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_data['transcript']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Bhavya\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Preprocessing of the data\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "import string\n",
    "import contractions\n",
    "\n",
    "# Download NLTK stopwords\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Define preprocessing function\n",
    "def preprocess_text(text):\n",
    "    # Lowercase\n",
    "    text = text.lower()\n",
    "    # Expand contractions\n",
    "    text = contractions.fix(text)\n",
    "    # Removing punctuation\n",
    "    text = ''.join([char for char in text if char not in string.punctuation])\n",
    "    # Removing stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    text = ' '.join([word for word in text.split() if word.lower() not in stop_words])\n",
    "    # # Stemming\n",
    "    # stemmer = PorterStemmer()\n",
    "    # text = ' '.join([stemmer.stem(word) for word in text.split()])\n",
    "    return text\n",
    "\n",
    "# Apply preprocessing function to 'transcript' column\n",
    "combined_data['transcript'] = combined_data['transcript'].apply(preprocess_text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenization of transcripts and encoding of labels\n",
    "\n",
    "combined_data['tokenized_transcript'] = combined_data['transcript'].apply(lambda x: word_tokenize(str(x).lower()))\n",
    "\n",
    "combined_data['label_encoded'] = combined_data['label'].map({'informative': 1, 'entertainment': 0})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Density Plot of Likes, Views and Comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Views\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.kdeplot(informational_data['view_count']/10**6, label='Informative', fill=True,)\n",
    "sns.kdeplot(entertainment_data['view_count']/10**6, label='Non-Informative', fill=True,)\n",
    "\n",
    "plt.xlabel('Views in millions')\n",
    "plt.ylabel('Density')\n",
    "plt.title('Density plot of Views')\n",
    "\n",
    "# plt.xlim(0,0.2*10**8)\n",
    "plt.xlim(0,15)\n",
    "plt.ylim(0,)\n",
    "\n",
    "plt.tight_layout() \n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Likes\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.kdeplot(informational_data['like_count']/10**3, label='Informative', fill=True)\n",
    "sns.kdeplot(entertainment_data['like_count']/10**3, label='Non-Informative', fill=True)\n",
    "\n",
    "plt.xlabel('Likes in thousands')\n",
    "plt.ylabel('Density')\n",
    "plt.title('Density plot of Likes')\n",
    "\n",
    "# plt.xlim(0,0.6*10**6)\n",
    "plt.xlim(0,600)\n",
    "plt.ylim(0,)\n",
    "\n",
    "plt.tight_layout() \n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Comments\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.kdeplot(informational_data['comment_count']/10**3, label='Informative', fill=True)\n",
    "sns.kdeplot(entertainment_data['comment_count']/10**3, label='Non-Informative', fill=True)\n",
    "\n",
    "plt.xlabel('Comments in thousands')\n",
    "plt.ylabel('Density')\n",
    "plt.title('Density plot of Comments')\n",
    "\n",
    "# plt.xlim(0,0.4*10**5)\n",
    "plt.xlim(0,40)\n",
    "plt.ylim(0,)\n",
    "\n",
    "plt.tight_layout() \n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation visualisation of likes, view and comments \n",
    "correlation_matrix = combined_data[['like_count', 'view_count', 'comment_count']].copy()\n",
    "correlation_matrix = correlation_matrix.corr()\n",
    "print(correlation_matrix)\n",
    "# Plot correlation matrix using heatmap\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=\".2f\", vmin=-1, vmax=1)\n",
    "plt.title('Correlation between metrics in the dataset')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation visualisation of likes, view and comments \n",
    "correlation_matrix = informational_data[['like_count', 'view_count', 'comment_count']].copy()\n",
    "correlation_matrix = correlation_matrix.corr()\n",
    "print(correlation_matrix)\n",
    "# Plot correlation matrix using heatmap\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=\".2f\", vmin=-1, vmax=1)\n",
    "plt.title('Correlation between metrics for informative content')\n",
    "plt.show()\n",
    "\n",
    "# Correlation visualisation of likes, view and comments \n",
    "correlation_matrix = entertainment_data[['like_count', 'view_count', 'comment_count']].copy()\n",
    "correlation_matrix = correlation_matrix.corr()\n",
    "print(correlation_matrix)\n",
    "# Plot correlation matrix using heatmap\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=\".2f\", vmin=-1, vmax=1)\n",
    "plt.title('Correlation between metrics for non-informative content')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Like-View Ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lvr = combined_data['like_count']/combined_data['view_count']\n",
    "lvr = lvr.dropna()\n",
    "\n",
    "mean_lvr = np.mean(lvr)\n",
    "median_lvr = np.median(lvr)\n",
    "std_dev_lvr = np.std(lvr)\n",
    "min_lvr = np.min(lvr)\n",
    "max_lvr = np.max(lvr)\n",
    "\n",
    "print(\"Descriptive Statistics for Like-to-View Ratio (LVR):\")\n",
    "print(\"Mean LVR:\", mean_lvr)\n",
    "print(\"Median LVR:\", median_lvr)\n",
    "print(\"Standard Deviation of LVR:\", std_dev_lvr)\n",
    "print(\"Minimum LVR:\", min_lvr)\n",
    "print(\"Maximum LVR:\", max_lvr)\n",
    "\n",
    "# Density plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.kdeplot(lvr, fill=True)\n",
    "plt.title('Density Plot of Like-to-View Ratio (LVR)')\n",
    "plt.xlabel('Like to View Ratio')\n",
    "plt.ylabel('Density')\n",
    "plt.xlim(0,0.25)\n",
    "plt.ylim(0,)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "informational_data_lvr = informational_data['like_count'] / informational_data ['view_count']\n",
    "entertainment_data_lvr = entertainment_data['like_count'] / entertainment_data ['view_count']\n",
    "informational_data['lvr'] = informational_data['like_count'] / informational_data ['view_count']\n",
    "entertainment_data['lvr'] = entertainment_data['like_count'] / entertainment_data ['view_count']\n",
    "\n",
    "informational_data_lvr = informational_data_lvr.dropna()\n",
    "entertainment_data_lvr = entertainment_data_lvr.dropna()\n",
    "\n",
    "informational_mean_lvr = np.mean(informational_data_lvr)\n",
    "informational_median_lvr = np.median(informational_data_lvr)\n",
    "print(f\"Informational:\\n Mean: {informational_mean_lvr} Median {informational_median_lvr}\")\n",
    "entertainment_mean_lvr = np.mean(entertainment_data_lvr)\n",
    "entertainment_median_lvr = np.median(entertainment_data_lvr)\n",
    "print(f\"Entertainment:\\n Mean: {entertainment_mean_lvr} Median {entertainment_median_lvr}\")\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.kdeplot(informational_data_lvr, label='Informative', fill=True,)\n",
    "sns.kdeplot(entertainment_data_lvr, label='Non-Informative', fill=True,)\n",
    "\n",
    "plt.xlabel('Like to View Ratio')\n",
    "plt.ylabel('Density')\n",
    "plt.title('Density plot of Like to View Ratio')\n",
    "\n",
    "plt.xlim(0,0.25)\n",
    "plt.ylim(0,)\n",
    "\n",
    "plt.tight_layout() \n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF Visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the TF-IDF vectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=5000, stop_words='english')\n",
    "\n",
    "# Fit the vectorizer and transform the transcript data\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(combined_data['transcript'])\n",
    "\n",
    "# Convert the TF-IDF matrix to a DataFrame\n",
    "tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=tfidf_vectorizer.get_feature_names_out())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF visualisations for informational and entertainment data\n",
    "entertainment_tfidf_df = tfidf_df.iloc[:len(entertainment_data)].copy()\n",
    "informational_tfidf_df = tfidf_df.iloc[len(entertainment_data):].copy()\n",
    "\n",
    "informational_tfidf_df['tf_idf_mean'] = informational_tfidf_df.apply(tf_idf_mean, axis=1)\n",
    "entertainment_tfidf_df['tf_idf_mean'] = entertainment_tfidf_df.apply(tf_idf_mean, axis=1)\n",
    "tfidf_df['tf_idf_mean'] = tfidf_df.apply(tf_idf_mean, axis=1)\n",
    "# Plot the density plot\n",
    "plt.figure(figsize=(10,6))\n",
    "# informational_tfidf_df['tf_idf_mean'].plot(kind='density', label = 'Informational')\n",
    "# entertainment_tfidf_df['tf_idf_mean'].plot(kind='density', label='Entertainment')\n",
    "sns.kdeplot(informational_tfidf_df['tf_idf_mean'], label='Informative', fill=True)\n",
    "sns.kdeplot(entertainment_tfidf_df['tf_idf_mean'], label='Non-Informative', fill=True)\n",
    "\n",
    "plt.title('TF-IDF Density Plot')\n",
    "plt.xlabel('TF-IDF Score')\n",
    "plt.ylabel('Density')\n",
    "plt.xlim(0, 0.2)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation matrix visualisation\n",
    "combined_data_tfidf = pd.concat([tfidf_df,combined_data],axis=1)\n",
    "# Merge the dataframes based on a common identifier\n",
    "correlation_matrix = combined_data_tfidf[['title','transcript', 'like_count', 'view_count', 'comment_count','tf_idf_mean','label_encoded']]\n",
    "correlation_matrix['like_view_ratio'] = correlation_matrix['like_count'] / correlation_matrix['view_count']\n",
    "correlation_matrix['comment_view_ratio'] = correlation_matrix['comment_count'] / correlation_matrix['view_count']\n",
    "correlation_matrix['comment_like_ratio'] = correlation_matrix['comment_count'] / correlation_matrix['like_count']\n",
    "\n",
    "# Compute correlation matrix\n",
    "correlation_matrix = correlation_matrix[['tf_idf_mean','like_view_ratio','label_encoded']].corr()\n",
    "print(correlation_matrix)\n",
    "# Plot correlation matrix using heatmap\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=\".2f\", vmin=-1, vmax=1)\n",
    "plt.title('TF-IDF Correlation Between Metrics')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Top 10 TF-IDF Visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the TF-IDF matrix as an array\n",
    "tfidf_array = tfidf_matrix.toarray()\n",
    "\n",
    "# Sort the TF-IDF values for each document\n",
    "sorted_tfidf = np.sort(tfidf_array, axis=1)\n",
    "\n",
    "# Top values\n",
    "top_10_values = sorted_tfidf[:, -10:]\n",
    "\n",
    "# Mean of top values\n",
    "top_10_mean_values = np.mean(top_10_values, axis=1)\n",
    "\n",
    "# Add column to dataframe\n",
    "tfidf_df['top_10_mean'] = top_10_mean_values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF visualisations for informational and entertainment data\n",
    "top10_entertainment_tfidf_df = tfidf_df.iloc[:len(entertainment_data)].copy()\n",
    "top10_informational_tfidf_df = tfidf_df.iloc[len(entertainment_data):].copy()\n",
    "\n",
    "# Plot the density plot\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.kdeplot(top10_entertainment_tfidf_df['top_10_mean'], label='Entertainment', fill=True)\n",
    "sns.kdeplot(top10_informational_tfidf_df['top_10_mean'], label='Informational', fill=True)\n",
    "plt.title('Top 10 TF-IDF Density Plot')\n",
    "plt.xlabel('Mean of top 10 TF-IDF Scores')\n",
    "plt.ylabel('Density')\n",
    "plt.xlim(0,)\n",
    "plt.ylim(0,)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation matrix visualisation\n",
    "combined_data_tfidf = pd.concat([tfidf_df,combined_data],axis=1)\n",
    "# Merge the dataframes based on a common identifier\n",
    "correlation_matrix = combined_data_tfidf[['title','transcript', 'like_count', 'view_count', 'comment_count','top_10_mean','label_encoded']]\n",
    "correlation_matrix['like_view_ratio'] = correlation_matrix['like_count'] / correlation_matrix['view_count']\n",
    "correlation_matrix['comment_view_ratio'] = correlation_matrix['comment_count'] / correlation_matrix['view_count']\n",
    "correlation_matrix['comment_like_ratio'] = correlation_matrix['comment_count'] / correlation_matrix['like_count']\n",
    "\n",
    "# Compute correlation matrix\n",
    "correlation_matrix = correlation_matrix[['top_10_mean','like_view_ratio','label_encoded']].corr()\n",
    "print(correlation_matrix)\n",
    "# Plot correlation matrix using heatmap\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=\".2f\", vmin=-1, vmax=1)\n",
    "plt.title('Top 10 TF-IDF Correlation Between Metrics')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "\n",
    "# ENTERTAINMENT\n",
    "\n",
    "# Get the feature names (words)\n",
    "feature_names = entertainment_tfidf_df.columns\n",
    "feature_names = [feature for feature in feature_names if feature != 'tf_idf_mean']\n",
    "\n",
    "entertainment_total_tfidf_scores = {}\n",
    "\n",
    "# Sum of Tf-idf scores of words\n",
    "for word in feature_names:\n",
    "    entertainment_total_tfidf_scores[word] = entertainment_tfidf_df[word].sum()\n",
    "\n",
    "# Sort the words based on tf-idf values\n",
    "sorted_tfidf_scores = dict(sorted(entertainment_total_tfidf_scores.items(), key=lambda x: x[1], reverse=True))\n",
    "# Get the top 100 words based on TF-IDF scores\n",
    "top_100_words = dict(sorted(sorted_tfidf_scores.items(), key=lambda item: item[1], reverse=True)[:100])\n",
    "\n",
    "# # Get the top 10 words based on TF-IDF scores\n",
    "# top_100_words = list(sorted_tfidf_scores.keys())[:100]\n",
    "\n",
    "# # Print out the top 10 words and their TF-IDF scores\n",
    "# print(\"Top 100 Words and their TF-IDF Scores:\")\n",
    "# for word in top_100_words:\n",
    "#     print(f\"{word}: {sorted_tfidf_scores[word]}\")\n",
    "# Create a word cloud\n",
    "# wordcloud = WordCloud(width=800, height=400, background_color='white').generate(' '.join(top_100_words))\n",
    "wordcloud = WordCloud(width=800, height=400, background_color='white')\n",
    "wordcloud.generate_from_frequencies(top_100_words)\n",
    "\n",
    "# Plot the word cloud\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "print('\\nWord Cloud of Top 100 words based on TF-IDF Scores for Entertainment Category')\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INFORMATIONAL\n",
    "\n",
    "# Get the feature names (words)\n",
    "feature_names = informational_tfidf_df.columns\n",
    "feature_names = [feature for feature in feature_names if feature != 'tf_idf_mean']\n",
    "\n",
    "info_total_tfidf_scores = {}\n",
    "\n",
    "# Iterate through each word and sum its TF-IDF scores across all documents\n",
    "for word in feature_names:\n",
    "    info_total_tfidf_scores[word] = informational_tfidf_df[word].sum()\n",
    "\n",
    "# Sort the words based on tf-idf values\n",
    "sorted_tfidf_scores = dict(sorted(info_total_tfidf_scores.items(), key=lambda x: x[1], reverse=True))\n",
    "# Get the top 100 words based on TF-IDF scores\n",
    "top_100_words = dict(sorted(sorted_tfidf_scores.items(), key=lambda item: item[1], reverse=True)[:100])\n",
    "# Get the top 10 words based on TF-IDF scores\n",
    "# top_100_words = list(sorted_tfidf_scores.keys())[:100]\n",
    "# # Print out the top 10 words and their TF-IDF scores\n",
    "# print(\"Top 100 Words and their TF-IDF Scores:\")\n",
    "# for word in top_100_words:\n",
    "#     print(f\"{word}: {sorted_tfidf_scores[word]}\")\n",
    "\n",
    "# Create a word cloud\n",
    "# wordcloud = WordCloud(width=800, height=400, background_color='white').generate(' '.join(top_100_words))\n",
    "wordcloud = WordCloud(width=800, height=400, background_color='white')\n",
    "wordcloud.generate_from_frequencies(top_100_words)\n",
    "# Plot the word cloud\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "print('\\nWord Cloud of Top 100 words based on TF-IDF Scores for Informational Category')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DOC2VEC Visualisation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the next section before running this section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Doc2Vec model which is created in the next phase\n",
    "model = Doc2Vec.load(\"./models/doc2vec_model\")\n",
    "\n",
    "combined_data_doc2vec = combined_data.copy()\n",
    "\n",
    "# Function to infer embeddings for a given tokenized transcript\n",
    "def infer_embeddings(tokenized_transcript, model):\n",
    "    return model.infer_vector(tokenized_transcript)\n",
    "\n",
    "# Apply the function\n",
    "combined_data_doc2vec['embeddings'] = combined_data_doc2vec['tokenized_transcript'].apply(lambda x: infer_embeddings(x, model))\n",
    "\n",
    "combined_data_doc2vec['embedding_mean'] = combined_data_doc2vec['embeddings'].apply(lambda x: pd.Series(x).mean())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation matrix visualisation\n",
    "\n",
    "# Merge the dataframes based on a common identifier\n",
    "correlation_matrix = combined_data_doc2vec[['title','transcript','embedding_mean', 'like_count', 'view_count', 'comment_count','label_encoded']]\n",
    "correlation_matrix.head()\n",
    "correlation_matrix['like_view_ratio'] = correlation_matrix['like_count'] / correlation_matrix['view_count']\n",
    "correlation_matrix['comment_view_ratio'] = correlation_matrix['comment_count'] / correlation_matrix['view_count']\n",
    "correlation_matrix['comment_like_ratio'] = correlation_matrix['comment_count'] / correlation_matrix['like_count']\n",
    "\n",
    "# Compute correlation matrix\n",
    "correlation_matrix = correlation_matrix[['embedding_mean','like_view_ratio','label_encoded']].corr()\n",
    "print(correlation_matrix)\n",
    "# Plot correlation matrix using heatmap\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=\".2f\", vmin=-1, vmax=1)\n",
    "plt.title('Doc2Vec Correlation Between Metrics')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Word2Vec visualisations for informational and entertainment data\n",
    "entertainment_doc2vec = combined_data_doc2vec.iloc[:len(entertainment_data)].copy()\n",
    "informational_doc2vec = combined_data_doc2vec.iloc[len(entertainment_data):].copy()\n",
    "\n",
    "# Scale the input data to [0, 1] range\n",
    "scaler = MinMaxScaler()\n",
    "informational_doc2vec_scaled = scaler.fit_transform(informational_doc2vec['embedding_mean'].values.reshape(-1, 1))\n",
    "entertainment_doc2vec_scaled = scaler.transform(entertainment_doc2vec['embedding_mean'].values.reshape(-1, 1))\n",
    "\n",
    "# Plot the density plot\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.kdeplot(informational_doc2vec_scaled.flatten(), label='Informative', fill=True)\n",
    "sns.kdeplot(entertainment_doc2vec_scaled.flatten(), label='Non-Informative', fill=True)\n",
    "\n",
    "plt.title('Doc2Vec Density Plot')\n",
    "plt.xlabel('Mean of Doc2Vec Embeddings ')\n",
    "plt.ylabel('Density')\n",
    "plt.xlim(0,)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CLASSIFICATION MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Tokenize transcripts\n",
    "# combined_data['tokenized_transcript'] = combined_data['transcript'].apply(lambda x: word_tokenize(str(x).lower()))\n",
    "\n",
    "# # Encode labels ('informative' and 'non-informative')\n",
    "# combined_data['label_encoded'] = combined_data['label'].map({'informative': 1, 'entertainment': 0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into features and labels\n",
    "X = combined_data['tokenized_transcript']\n",
    "y = combined_data['label_encoded']\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Tag documents with unique identifiers\n",
    "tagged_data = [TaggedDocument(words=doc, tags=[i]) for i, doc in enumerate(X_train)]\n",
    "\n",
    "# Load the Doc2Vec model\n",
    "model = Doc2Vec.load(\"./models/doc2vec_model\")\n",
    "\n",
    "# # Train Doc2Vec model\n",
    "# model = Doc2Vec(vector_size=100, window=5, min_count=1, workers=4, epochs=20)\n",
    "# model.build_vocab(tagged_data)\n",
    "# model.train(tagged_data, total_examples=model.corpus_count, epochs=model.epochs)\n",
    "\n",
    "# Get document embeddings for training and testing sets\n",
    "X_train_embeddings = [model.infer_vector(doc) for doc in X_train]\n",
    "X_test_embeddings = [model.infer_vector(doc) for doc in X_test]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring classifiers for the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9529667149059334\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    0.96373   0.95917   0.96145      1690\n",
      "           1    0.93623   0.94320   0.93970      1074\n",
      "\n",
      "    accuracy                        0.95297      2764\n",
      "   macro avg    0.94998   0.95119   0.95058      2764\n",
      "weighted avg    0.95305   0.95297   0.95300      2764\n",
      "\n",
      "Confusion Matrix:\n",
      "[[1621   69]\n",
      " [  61 1013]]\n"
     ]
    }
   ],
   "source": [
    "# Classifier for the model\n",
    "# Train logistic regression model\n",
    "clf_logreg = LogisticRegression()\n",
    "clf_logreg.fit(X_train_embeddings, y_train)\n",
    "# Predict labels for testing set\n",
    "y_pred_lr = clf_logreg.predict(X_test_embeddings)\n",
    "\n",
    "# Evaluate model performance\n",
    "accuracy = accuracy_score(y_test, y_pred_lr)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(classification_report(y_test, y_pred_lr, digits=5))\n",
    "\n",
    "# Confusion Matrix\n",
    "conf_matrix = confusion_matrix(y_test, y_pred_lr)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9678002894356006\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    0.97395   0.97337   0.97366      1690\n",
      "           1    0.95814   0.95903   0.95859      1074\n",
      "\n",
      "    accuracy                        0.96780      2764\n",
      "   macro avg    0.96604   0.96620   0.96612      2764\n",
      "weighted avg    0.96781   0.96780   0.96780      2764\n",
      "\n",
      "Confusion Matrix:\n",
      "[[1645   45]\n",
      " [  44 1030]]\n"
     ]
    }
   ],
   "source": [
    "# Train SVM model\n",
    "clf_svm = SVC()\n",
    "clf_svm.fit(X_train_embeddings, y_train)\n",
    "# Predict labels for testing set using SVM\n",
    "y_pred_svm = clf_svm.predict(X_test_embeddings)\n",
    "\n",
    "# Evaluate model performance\n",
    "accuracy = accuracy_score(y_test, y_pred_svm)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(classification_report(y_test, y_pred_svm, digits=5))\n",
    "\n",
    "# Confusion Matrix\n",
    "conf_matrix = confusion_matrix(y_test, y_pred_svm)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9569464544138929\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    0.96288   0.96686   0.96487      1690\n",
      "           1    0.94752   0.94134   0.94442      1074\n",
      "\n",
      "    accuracy                        0.95695      2764\n",
      "   macro avg    0.95520   0.95410   0.95464      2764\n",
      "weighted avg    0.95691   0.95695   0.95692      2764\n",
      "\n",
      "Confusion Matrix:\n",
      "[[1634   56]\n",
      " [  63 1011]]\n"
     ]
    }
   ],
   "source": [
    "# Train Random Forest model\n",
    "clf_rf = RandomForestClassifier()\n",
    "clf_rf.fit(X_train_embeddings, y_train)\n",
    "# Predict labels for testing set using Random Forest\n",
    "y_pred_rf = clf_rf.predict(X_test_embeddings)\n",
    "\n",
    "# Evaluate model performance\n",
    "accuracy = accuracy_score(y_test, y_pred_rf)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(classification_report(y_test, y_pred_rf, digits=5))\n",
    "\n",
    "# Confusion Matrix\n",
    "conf_matrix = confusion_matrix(y_test, y_pred_rf)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.821273516642547\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    0.95098   0.74615   0.83621      1690\n",
      "           1    0.70167   0.93948   0.80334      1074\n",
      "\n",
      "    accuracy                        0.82127      2764\n",
      "   macro avg    0.82632   0.84282   0.81978      2764\n",
      "weighted avg    0.85411   0.82127   0.82344      2764\n",
      "\n",
      "Confusion Matrix:\n",
      "[[1261  429]\n",
      " [  65 1009]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "# Train Gaussian NB model\n",
    "clf_gnb = GaussianNB()\n",
    "clf_gnb.fit(X_train_embeddings, y_train)\n",
    "# Predict labels for testing set using Multinomial NB\n",
    "y_pred_nb = clf_gnb.predict(X_test_embeddings)\n",
    "\n",
    "# Evaluate model performance\n",
    "accuracy = accuracy_score(y_test, y_pred_nb)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(classification_report(y_test, y_pred_nb, digits=5))\n",
    "\n",
    "# Confusion Matrix\n",
    "conf_matrix = confusion_matrix(y_test, y_pred_nb)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the Doc2Vec model\n",
    "model.save(\"./models/doc2vec_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LIKE PREDICTION MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# New dataset\n",
    "# yt_lvr_data = yt_data.copy()\n",
    "# yt_lvr_data.drop(columns=['video_id','categoryId','dislikes'], inplace=True)\n",
    "\n",
    "# lvr_data = pd.concat([ted_data,yt_educational_data,vox_data,yt_lvr_data], ignore_index=True, join='inner')\n",
    "lvr_data = combined_data.copy()\n",
    "# Drop rows where view_count or like_count is 0 or NaN\n",
    "lvr_data = lvr_data.dropna(subset=['view_count', 'like_count'])\n",
    "lvr_data = lvr_data[(lvr_data['view_count'] != 0) | (lvr_data['like_count'] != 0)]\n",
    "lvr_data = lvr_data.reset_index(drop=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DOC2VEC Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = lvr_data['tokenized_transcript']\n",
    "y = lvr_data['like_count']/lvr_data['view_count']\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# # Tag documents with unique identifiers\n",
    "# tagged_data = [TaggedDocument(words=doc, tags=[i]) for i, doc in enumerate(X_train)]\n",
    "\n",
    "# Load the Doc2Vec model\n",
    "model = Doc2Vec.load(\"./models/doc2vec_model\")\n",
    "# # Train Doc2Vec lvr_model\n",
    "# lvr_model = Doc2Vec(vector_size=100, window=5, min_count=1, workers=4, epochs=20)\n",
    "# lvr_model.build_vocab(tagged_data)\n",
    "# lvr_model.train(tagged_data, total_examples=lvr_model.corpus_count, epochs=lvr_model.epochs)\n",
    "\n",
    "# Get document embeddings for training and testing sets\n",
    "X_train_embeddings = [model.infer_vector(doc) for doc in X_train]\n",
    "X_test_embeddings = [model.infer_vector(doc) for doc in X_test]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EXPLORING CLASSIFIERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "# Train a linear regression clf\n",
    "clf = LinearRegression()\n",
    "clf.fit(X_train_embeddings, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = clf.predict(X_test_embeddings)\n",
    "\n",
    "print(\"Linear Regression using DOC2VEC\")\n",
    "# Evaluate the clf\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "print(\"Mean Absolute Error:\", mae)\n",
    "\n",
    "# Evaluate model performance\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(\"Mean Squared Error:\", mse)\n",
    "\n",
    "rmse = np.sqrt(mse)\n",
    "print(\"Root Mean Squared Error:\", rmse)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "# Initialize Ridge Regression model\n",
    "ridge_model = Ridge()\n",
    "\n",
    "# Train Ridge Regression model\n",
    "ridge_model.fit(X_train_embeddings, y_train)\n",
    "\n",
    "# Predict engagement ratios for the testing set\n",
    "y_pred = ridge_model.predict(X_test_embeddings)\n",
    "\n",
    "\n",
    "print(\"Ridge Regression using DOC2VEC\")\n",
    "# Evaluate the clf\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "print(\"Mean Absolute Error:\", mae)\n",
    "\n",
    "# Evaluate model performance\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(\"Mean Squared Error:\", mse)\n",
    "\n",
    "rmse = np.sqrt(mse)\n",
    "print(\"Root Mean Squared Error:\", rmse)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## USING TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # TF-IDF\n",
    "# # Initialize the TF-IDF vectorizer\n",
    "# lvr_tfidf_vectorizer = TfidfVectorizer(max_features=5000, stop_words='english')\n",
    "\n",
    "# # Fit the vectorizer and transform the transcript data\n",
    "# lvr_tfidf_matrix = lvr_tfidf_vectorizer.fit_transform(lvr_data['transcript'])\n",
    "\n",
    "# # Convert the TF-IDF matrix to a DataFrame\n",
    "# lvr_tfidf_df = pd.DataFrame(lvr_tfidf_matrix.toarray(), columns=lvr_tfidf_vectorizer.get_feature_names_out())\n",
    "\n",
    "# lvr_tfidf_df['tf_idf_mean'] = lvr_tfidf_df.apply(tf_idf_mean, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = lvr_data['transcript']\n",
    "y = lvr_data['like_count']/lvr_data['view_count']\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Convert text data to numerical features using TF-IDF\n",
    "lvr_tfidf_vectorizer = TfidfVectorizer(max_features=5000, stop_words='english')  # You can adjust max_features as needed\n",
    "X_train_tfidf = lvr_tfidf_vectorizer.fit_transform(X_train)\n",
    "X_test_tfidf = lvr_tfidf_vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EXPLORING CLASSIFIERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a linear regression clf\n",
    "clf = LinearRegression()\n",
    "clf.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = clf.predict(X_test_tfidf)\n",
    "\n",
    "print(\"Linear Regression using TF-IDF\")\n",
    "# Evaluate the clf\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "print(\"Mean Absolute Error:\", mae)\n",
    "\n",
    "# Evaluate model performance\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(\"Mean Squared Error:\", mse)\n",
    "\n",
    "rmse = np.sqrt(mse)\n",
    "print(\"Root Mean Squared Error:\", rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "# Initialize Ridge Regression model\n",
    "ridge_model = Ridge()\n",
    "\n",
    "# Train Ridge Regression model\n",
    "ridge_model.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# Predict engagement ratios for the testing set\n",
    "y_pred = ridge_model.predict(X_test_tfidf)\n",
    "\n",
    "print(\"Ridge Regression using TF-IDF\")\n",
    "# Evaluate the clf\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "print(\"Mean Absolute Error:\", mae)\n",
    "\n",
    "# Evaluate model performance\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(\"Mean Squared Error:\", mse)\n",
    "\n",
    "rmse = np.sqrt(mse)\n",
    "print(\"Root Mean Squared Error:\", rmse)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TESTING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and preprocess the new transcript\n",
    "with open(\"./transcript_to_test.txt\", \"r\") as file:\n",
    "    new_transcript = file.read().lower()\n",
    "tokenized_new_transcript = word_tokenize(new_transcript)\n",
    "\n",
    "# Load the Doc2Vec model\n",
    "model = Doc2Vec.load(\"./models/doc2vec_model\")\n",
    "\n",
    "# Infer Doc2Vec embedding for the new transcript\n",
    "embedding = model.infer_vector(tokenized_new_transcript)\n",
    "\n",
    "# Predict the label using the trained SVM classification model\n",
    "predicted_label = clf_svm.predict([embedding])[0]  \n",
    "\n",
    "# Map the predicted label back to its original form\n",
    "predicted_label_original = {1: 'informative', 0: 'non-informative'}.get(predicted_label)\n",
    "\n",
    "print(\"Predicted Label:\", predicted_label_original)\n",
    "\n",
    "# Predict probabilities for each class for testing set\n",
    "y_probabilities = clf_svm.predict_proba([embedding])[0]\n",
    "\n",
    "print(\"Predicted Probabilities for each class for the testing set:\")\n",
    "print(y_probabilities)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
